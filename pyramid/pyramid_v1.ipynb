{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, out_channels, \n",
    "                            kernel_size=kernel_size, stride=stride)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, L, C)\n",
    "        x = self.norm(x)\n",
    "        return x, H, W\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Linear(dim, dim * 2)\n",
    "        self.sr_ratio = sr_ratio\n",
    "        \n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, \n",
    "                               kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, \n",
    "                            self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            x_ = self.norm(x_)\n",
    "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, \n",
    "                                   self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        else:\n",
    "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, \n",
    "                                  self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, sr_ratio=1, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads, sr_ratio)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = Mlp(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = x + self.attn(self.norm1(x), H, W)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PVT224(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "        # Stage configurations: [embed_dim, num_heads, sr_ratio, num_blocks]\n",
    "        stage_cfgs = [\n",
    "            [64, 1, 8, 2],   # Stage 1 (56x56)\n",
    "            [128, 2, 4, 2],  # Stage 2 (28x28)\n",
    "            [320, 5, 2, 2],  # Stage 3 (14x14)\n",
    "            [512, 8, 1, 2],  # Stage 4 (7x7)\n",
    "        ]\n",
    "        \n",
    "        # Initial patch embedding\n",
    "        self.patch_embed1 = PatchEmbed(3, 64, kernel_size=4, stride=4)\n",
    "        \n",
    "        # Subsequent stages\n",
    "        self.stages = nn.ModuleList()\n",
    "        self.patch_embeds = nn.ModuleList()\n",
    "        \n",
    "        for i, (dim, heads, sr, blocks) in enumerate(stage_cfgs):\n",
    "            if i > 0:\n",
    "                self.patch_embeds.append(\n",
    "                    PatchEmbed(stage_cfgs[i-1][0], dim, kernel_size=2, stride=2)\n",
    "            stage = nn.Sequential(*[\n",
    "                TransformerBlock(dim, heads, sr, mlp_ratio=4)\n",
    "                for _ in range(blocks)\n",
    "            ])\n",
    "            self.stages.append(stage)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(stage_cfgs[-1][0])\n",
    "        self.head = nn.Linear(stage_cfgs[-1][0], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        \n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if i > 0:\n",
    "                x = x.permute(0, 2, 1).reshape(B, -1, H, W)\n",
    "                x, H, W = self.patch_embeds[i-1](x)\n",
    "            \n",
    "            for blk in stage:\n",
    "                x = blk(x, H, W)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# Model Summary\n",
    "model = PVT224()\n",
    "summary(model, input_size=(1, 3, 224, 224))\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal Parameters: {total_params/1e6:.2f}M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
