{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torchvision.transforms as transforms\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "\n",
    "image_size =224\n",
    "tiny_transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.RandomCrop(image_size, padding=5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "tiny_transform_val = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "tiny_transform_test = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "train_loader, val_loader, test_loader = get_tinyimagenet_dataloaders(\n",
    "                                                    data_dir = '../datasets',\n",
    "                                                    transform_train=tiny_transform_train,\n",
    "                                                    transform_val=tiny_transform_val,\n",
    "                                                    transform_test=tiny_transform_test,\n",
    "                                                    batch_size=64,\n",
    "                                                    image_size=image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 117,898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 340\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete and model saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 327\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# ----------- Training Loop -----------\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 327\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    329\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[5], line 268\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    266\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    267\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 268\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    271\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "############################################\n",
    "# 1. Low-Rank Linear Projection Module\n",
    "############################################\n",
    "\n",
    "class LowRankLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer whose weight matrix is factorized as A * B,\n",
    "    reducing parameters while approximating a full linear projection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=16, bias=True):\n",
    "        super().__init__()\n",
    "        self.A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.B = nn.Linear(rank, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.B(self.A(x))\n",
    "\n",
    "############################################\n",
    "# 2. Low-Rank Self-Attention Module\n",
    "############################################\n",
    "\n",
    "class LowRankSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention using low-rank factorization for Q, K, and V projections.\n",
    "    Expects input tokens of shape (B, N, C).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, rank=16):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = LowRankLinear(dim, dim, rank=rank)\n",
    "        self.k_proj = LowRankLinear(dim, dim, rank=rank)\n",
    "        self.v_proj = LowRankLinear(dim, dim, rank=rank)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, C)\n",
    "        B, N, C = x.shape\n",
    "        q = self.q_proj(x).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, num_heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v  # (B, num_heads, N, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "############################################\n",
    "# 3. Adaptive Window Attention with Token Pruning\n",
    "############################################\n",
    "\n",
    "class AdaptiveWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Partitions a spatial feature map into windows with a dynamically computed window size,\n",
    "    applies low-rank self-attention within each window, and then prunes tokens.\n",
    "    \n",
    "    Input is expected in (B, H, W, C) format.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, rank=16,\n",
    "                 min_window=4, max_window=8, token_keep_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.rank = rank\n",
    "        self.min_window = min_window\n",
    "        self.max_window = max_window\n",
    "        self.token_keep_ratio = token_keep_ratio\n",
    "\n",
    "        # Low-rank self-attention for each window.\n",
    "        self.attn = LowRankSelfAttention(dim, num_heads=num_heads, rank=rank)\n",
    "\n",
    "        # Gating module: predicts a scalar to decide the window size.\n",
    "        self.window_gate = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # input: (B, C, H, W)\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "        # Token scoring layer for pruning.\n",
    "        self.token_score = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, H, W, C)\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        # ---- Dynamic Window Size ----\n",
    "        # Compute a gating scalar from global features.\n",
    "        x_perm = x.permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
    "        gate_val = torch.sigmoid(self.window_gate(x_perm))  # (B, 1) with values in (0,1)\n",
    "        # Average the gate values over the batch.\n",
    "        gate_scalar = gate_val.mean().item()\n",
    "        dynamic_window = int(round(self.min_window + gate_scalar * (self.max_window - self.min_window)))\n",
    "        dynamic_window = max(self.min_window, min(dynamic_window, self.max_window))\n",
    "        # Pad H and W so they are divisible by dynamic_window.\n",
    "        pad_h = (dynamic_window - H % dynamic_window) % dynamic_window\n",
    "        pad_w = (dynamic_window - W % dynamic_window) % dynamic_window\n",
    "        if pad_h or pad_w:\n",
    "            # Pad last two dimensions (H, W); using (left, right, top, bottom) padding.\n",
    "            x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "        H_new, W_new = x.shape[1], x.shape[2]\n",
    "\n",
    "        # ---- Partition into Windows ----\n",
    "        # Reshape: (B, H_new/dynamic_window, dynamic_window, W_new/dynamic_window, dynamic_window, C)\n",
    "        x_windows = x.view(B, H_new // dynamic_window, dynamic_window,\n",
    "                           W_new // dynamic_window, dynamic_window, C)\n",
    "        # Permute to (B, num_windows_H, num_windows_W, dynamic_window, dynamic_window, C)\n",
    "        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        num_windows = (H_new // dynamic_window) * (W_new // dynamic_window)\n",
    "        # Merge batch and window dims: (B*num_windows, window_area, C)\n",
    "        x_windows = x_windows.view(B * num_windows, dynamic_window * dynamic_window, C)\n",
    "\n",
    "        # ---- Local Self-Attention ----\n",
    "        x_windows = self.attn(x_windows)  # (B*num_windows, window_area, C)\n",
    "\n",
    "        # ---- Token Pruning ----\n",
    "        scores = self.token_score(x_windows).squeeze(-1)  # (B*num_windows, window_area)\n",
    "        N = x_windows.shape[1]\n",
    "        k = max(1, int(self.token_keep_ratio * N))  # number of tokens to keep per window\n",
    "        # Get top-k token indices per window.\n",
    "        _, topk_indices = torch.topk(scores, k, dim=1)\n",
    "        # Gather tokens\n",
    "        topk_indices_expanded = topk_indices.unsqueeze(-1).expand(-1, -1, C)\n",
    "        pruned_tokens = torch.gather(x_windows, dim=1, index=topk_indices_expanded)  # (B*num_windows, k, C)\n",
    "        # Merge windows back: shape becomes (B, num_windows*k, C)\n",
    "        pruned_tokens = pruned_tokens.view(B, num_windows * k, C)\n",
    "        return pruned_tokens\n",
    "\n",
    "############################################\n",
    "# 4. Shared Swin Block with Weight Sharing\n",
    "############################################\n",
    "\n",
    "class SharedSwinBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer block that applies adaptive window attention and an MLP.\n",
    "    This module is designed to be reâ€‘used (i.e. its weights are shared) across multiple layers.\n",
    "    \n",
    "    Accepts input either as a spatial grid (B, H, W, C) or as a sequence of tokens (B, N, C).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0,\n",
    "                 num_heads=8, rank=16, min_window=4, max_window=8, token_keep_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = AdaptiveWindowAttention(dim, num_heads=num_heads, rank=rank,\n",
    "                                              min_window=min_window, max_window=max_window,\n",
    "                                              token_keep_ratio=token_keep_ratio)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If input is spatial: (B, H, W, C)\n",
    "        if x.dim() == 4:\n",
    "            B, H, W, C = x.shape\n",
    "            x_flat = x.view(B, H * W, C)\n",
    "            x_norm = self.norm1(x_flat)\n",
    "            x_spatial = x_norm.view(B, H, W, C)\n",
    "            # Apply adaptive window attention (returns pruned tokens)\n",
    "            attn_out = self.attn(x_spatial)  # shape: (B, N', C)\n",
    "            # Apply MLP on pruned tokens (using another normalization)\n",
    "            attn_out = attn_out + self.mlp(self.norm2(attn_out))\n",
    "            # (Note: Residual connection is applied inside the MLP branch only.)\n",
    "            return attn_out\n",
    "        elif x.dim() == 3:\n",
    "            # Input already in token form: (B, N, C)\n",
    "            residual = x\n",
    "            x = self.norm1(x)\n",
    "            x = x + self.mlp(self.norm2(x))\n",
    "            return x + residual\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must be 3D or 4D.\")\n",
    "\n",
    "############################################\n",
    "# 5. Patch Embedding Module\n",
    "############################################\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits an image into patches and projects them to a desired embedding dimension.\n",
    "    Input: (B, in_chans, img_size, img_size)\n",
    "    Output: (B, H, W, embed_dim) with H = W = img_size/patch_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, H, W)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, embed_dim)\n",
    "        return x\n",
    "\n",
    "############################################\n",
    "# 6. Overall SwinLite Model\n",
    "############################################\n",
    "\n",
    "class SwinLite(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Swin Transformer variant that:\n",
    "      - Embeds image patches,\n",
    "      - Applies a shared (weight-shared) transformer block multiple times,\n",
    "      - Uses adaptive window attention with token pruning and low-rank projections,\n",
    "      - Outputs a classification score.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3,\n",
    "                 num_classes=1000, embed_dim=96, depth=4,\n",
    "                 mlp_ratio=4.0, drop=0.0, num_heads=8, rank=16,\n",
    "                 min_window=4, max_window=8, token_keep_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,\n",
    "                                      in_chans=in_chans, embed_dim=embed_dim)\n",
    "        # Create one shared block to be used repeatedly.\n",
    "        self.shared_block = SharedSwinBlock(dim=embed_dim, mlp_ratio=mlp_ratio,\n",
    "                                            drop=drop, num_heads=num_heads, rank=rank,\n",
    "                                            min_window=min_window, max_window=max_window,\n",
    "                                            token_keep_ratio=token_keep_ratio)\n",
    "        self.depth = depth\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, img_size, img_size)\n",
    "        x = self.patch_embed(x)  # (B, H, W, embed_dim)\n",
    "        # Apply the shared block repeatedly.\n",
    "        for i in range(self.depth):\n",
    "            x = self.shared_block(x)\n",
    "            # After the first block, x becomes a sequence: (B, N, embed_dim)\n",
    "        x = self.norm(x)\n",
    "        # Global average pooling over tokens.\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "############################################\n",
    "# 7. Training Code for Tiny ImageNet\n",
    "############################################\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def main():\n",
    "    # ----------- Configuration -----------\n",
    "    num_classes = 200  # Tiny ImageNet has 200 classes.\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "\n",
    "    # ----------- Device -----------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ----------- Model, Loss, Optimizer, Scheduler -----------\n",
    "    model = SwinLite(img_size=224, patch_size=4, in_chans=3, num_classes=num_classes,\n",
    "                     embed_dim=96, depth=4, mlp_ratio=4.0, drop=0.1,\n",
    "                     num_heads=8, rank=16, min_window=4, max_window=8, token_keep_ratio=0.7)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    # Print the number of trainable parameters.\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters: {num_params:,}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # ----------- Training Loop -----------\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save the trained model.\n",
    "    torch.save(model.state_dict(), \"swinlite_tiny_imagenet.pth\")\n",
    "    print(\"Training complete and model saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
