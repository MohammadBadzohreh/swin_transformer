{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "\n",
    "image_size =224\n",
    "tiny_transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.RandomCrop(image_size, padding=5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "tiny_transform_val = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "tiny_transform_test = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "train_loader, val_loader, test_loader = get_tinyimagenet_dataloaders(\n",
    "                                                    data_dir = '../datasets',\n",
    "                                                    transform_train=tiny_transform_train,\n",
    "                                                    transform_val=tiny_transform_val,\n",
    "                                                    transform_test=tiny_transform_test,\n",
    "                                                    batch_size=64,\n",
    "                                                    image_size=image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    # -------------------------------\n",
    "    # Hyperparameters\n",
    "    # -------------------------------\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "    lr = 5e-4\n",
    "    num_workers = 4\n",
    "    image_size = 224  # We'll upscale from 64x64 to 224x224 for Swin\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Create the Swin Transformer Model\n",
    "    # -------------------------------\n",
    "    # timm model variants for Swin:\n",
    "    #   - swin_tiny_patch4_window7_224\n",
    "    #   - swin_small_patch4_window7_224\n",
    "    #   - swin_base_patch4_window7_224\n",
    "    #   - ...\n",
    "    # Here, we use 'swin_tiny_patch4_window7_224' with 200 classes.\n",
    "    model = timm.create_model(\n",
    "        'swin_tiny_patch4_window7_224',\n",
    "        pretrained=False,\n",
    "        num_classes=200  # Tiny ImageNet has 200 classes\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Loss and Optimizer\n",
    "    # -------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Training Loop\n",
    "    # -------------------------------\n",
    "    best_val_acc = 0.0\n",
    "    best_model_path = \"best_swin_tiny_imagenet.pth\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Checkpoint\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  New best model saved with accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"Training finished. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Best model is saved at {best_model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
