{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of parameters: 28801400\n",
      "===== EPOCH 1 / 100 =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 634\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 634\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 611\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 611\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    612\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;66;03m# Step the scheduler after each epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 522\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    520\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    521\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 522\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    525\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "\n",
    "#############################################################################\n",
    "# 1. TinyImageNet Dataloader (If you have your own loader, use that instead)\n",
    "#############################################################################\n",
    "\n",
    "# If you have a custom TinyImageNet_loader.py, you can import the function\n",
    "# get_tinyimagenet_dataloaders. Otherwise, here's a simple reference using\n",
    "# torchvision's ImageFolder (assuming your data is structured like:\n",
    "#\n",
    "# data_dir/\n",
    "#   train/\n",
    "#       class_1/\n",
    "#           img1.jpeg\n",
    "#           ...\n",
    "#       class_2/\n",
    "#           img1.jpeg\n",
    "#           ...\n",
    "#   val/\n",
    "#       class_1/\n",
    "#           img1.jpeg\n",
    "#           ...\n",
    "#       ...\n",
    "#   test/   (if you have a separate test folder)\n",
    "#       class_1/\n",
    "#           ...\n",
    "#\n",
    "# If your folder structure is different, adjust accordingly.\n",
    "#\n",
    "# For the official TinyImageNet, you need to reshape the val folder\n",
    "# by class. That means placing each val image into subfolders named\n",
    "# by their label. For many standard distributions, this is already done.\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_size = 224\n",
    "tiny_transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.08, 1.0)),  # stronger random crop\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "tiny_transform_val = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "tiny_transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "train_loader, val_loader, test_loader = get_tinyimagenet_dataloaders(\n",
    "                                                    data_dir = '../datasets',\n",
    "                                                    transform_train=tiny_transform_train,\n",
    "                                                    transform_val=tiny_transform_val,\n",
    "                                                    transform_test=tiny_transform_test,\n",
    "                                                    batch_size=64,\n",
    "                                                    image_size=image_size)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# 3. Loaders\n",
    "#############################################################################\n",
    "\n",
    "data_dir = \"../datasets\"  # Adjust this path to your TinyImageNet folder\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = get_tinyimagenet_dataloaders(\n",
    "    data_dir=data_dir,\n",
    "    transform_train=tiny_transform_train,\n",
    "    transform_val=tiny_transform_val,\n",
    "    transform_test=tiny_transform_test,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "#############################################################################\n",
    "# 4. Squeeze-and-Excitation (SE) Block\n",
    "#############################################################################\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block for channel attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, reduction=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim // reduction)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(dim // reduction, dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, C)\n",
    "        b, n, c = x.shape\n",
    "        # Squeeze\n",
    "        x_mean = x.mean(dim=1)   # (B, C)\n",
    "        # Excitation\n",
    "        y = self.fc1(x_mean)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y)\n",
    "        y = y.unsqueeze(1)  # (B, 1, C)\n",
    "        return x * y        # broadcast over tokens\n",
    "\n",
    "#############################################################################\n",
    "# 5. Depthwise-Separable MLP\n",
    "#############################################################################\n",
    "\n",
    "class DSConvMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise-Separable MLP + optional Squeeze-and-Excitation\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio=4.0, dropout=0.0, use_se=True):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "\n",
    "        # Depthwise 1D Conv (across token dimension)\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            groups=hidden_dim\n",
    "        )\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.use_se = use_se\n",
    "        if use_se:\n",
    "            self.se = SEBlock(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, C)\n",
    "        x_fc1 = self.fc1(x)  # (B, N, hidden_dim)\n",
    "        x_fc1 = self.act(x_fc1)\n",
    "        x_fc1 = self.drop(x_fc1)\n",
    "\n",
    "        b, n, h = x_fc1.shape\n",
    "        # Depthwise convolution across N dimension\n",
    "        x_dw = x_fc1.permute(0, 2, 1)   # (B, hidden_dim, N)\n",
    "        x_dw = self.depthwise(x_dw)     # (B, hidden_dim, N)\n",
    "        x_dw = x_dw.permute(0, 2, 1)    # (B, N, hidden_dim)\n",
    "\n",
    "        x_dw = self.act(x_dw)\n",
    "        x_dw = self.drop(x_dw)\n",
    "\n",
    "        x_fc2 = self.fc2(x_dw)  # (B, N, dim)\n",
    "        x_fc2 = self.drop(x_fc2)\n",
    "\n",
    "        if self.use_se:\n",
    "            x_fc2 = self.se(x_fc2)\n",
    "\n",
    "        return x_fc2\n",
    "\n",
    "#############################################################################\n",
    "# 6. Window Partition / Reverse\n",
    "#############################################################################\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    x: (B, H, W, C)\n",
    "    return: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(\n",
    "        B,\n",
    "        H // window_size, window_size,\n",
    "        W // window_size, window_size,\n",
    "        C\n",
    "    )\n",
    "    # reorder => (B, num_win_h, num_win_w, wsize, wsize, C) => (num_win*B, wsize, wsize, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    windows = windows.view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    windows: (num_windows*B, window_size, window_size, C)\n",
    "    return: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B_ = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(\n",
    "        B_,\n",
    "        H // window_size,\n",
    "        W // window_size,\n",
    "        window_size,\n",
    "        window_size,\n",
    "        -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    x = x.view(B_, H, W, -1)\n",
    "    return x\n",
    "\n",
    "#############################################################################\n",
    "# 7. WindowAttention\n",
    "#############################################################################\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Window-based Multi-Head Self-Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (num_windows*B, window_size*window_size, C)\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        # Project QKV\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B_, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B_, heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = attn @ v  # (B_, heads, N, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(B_, N, C)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "#############################################################################\n",
    "# 8. SwinLite Block\n",
    "#############################################################################\n",
    "\n",
    "class SwinLiteBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Swin-like block. Shift or no-shift, window-based attention + DSConvMLP.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.0,\n",
    "        use_se=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "\n",
    "        if min(input_resolution) > window_size:\n",
    "            self.shift_size = shift_size\n",
    "        else:\n",
    "            self.shift_size = 0\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, window_size=self.window_size, num_heads=num_heads)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = DSConvMLP(dim=dim, mlp_ratio=mlp_ratio, dropout=dropout, use_se=use_se)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, H*W, C).\n",
    "        We'll reshape -> (B, H, W, C), do window attn, then reshape back.\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, f\"Input feature has wrong size {L} != {H*W}\"\n",
    "\n",
    "        # (1) Norm + Reshape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # (2) Shift if needed\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # (3) Window partition\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # (nW*B, wsize, wsize, C)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # (nW*B, wsize^2, C)\n",
    "\n",
    "        # (4) Window Attention\n",
    "        attn_windows = self.attn(x_windows)\n",
    "\n",
    "        # (5) Merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        # (6) Reverse shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        # (7) Flatten back\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + x  # skip\n",
    "\n",
    "        # (8) MLP\n",
    "        shortcut2 = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = shortcut2 + x\n",
    "\n",
    "        return x\n",
    "\n",
    "#############################################################################\n",
    "# 9. SwinLite Model (adjusted for better capacity)\n",
    "#############################################################################\n",
    "\n",
    "class SwinLite(nn.Module):\n",
    "    \"\"\"\n",
    "    A slightly bigger Swin-like model for Tiny ImageNet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        num_classes=200,\n",
    "        embed_dim=96,         # Increased from 48 -> 96 for better capacity\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],  # Standard Swin-T-like ratios\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.0,\n",
    "        use_se=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        # resolution after patch embedding\n",
    "        patches_resolution = (\n",
    "            image_size // patch_size,\n",
    "            image_size // patch_size\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        dim = embed_dim\n",
    "\n",
    "        for i in range(len(depths)):\n",
    "            stage = self._make_stage(\n",
    "                dim=dim,\n",
    "                input_resolution=patches_resolution,\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout,\n",
    "                use_se=use_se,\n",
    "                downsample=(i < len(depths) - 1)\n",
    "            )\n",
    "            self.layers.append(stage)\n",
    "\n",
    "            # If downsampling, the resolution is halved, channels doubled\n",
    "            if i < len(depths) - 1:\n",
    "                patches_resolution = (\n",
    "                    patches_resolution[0] // 2,\n",
    "                    patches_resolution[1] // 2\n",
    "                )\n",
    "                dim *= 2\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_stage(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio,\n",
    "        dropout,\n",
    "        use_se,\n",
    "        downsample\n",
    "    ):\n",
    "        blocks = []\n",
    "        for d in range(depth):\n",
    "            shift_size = 0 if (d % 2 == 0) else window_size // 2\n",
    "            blocks.append(\n",
    "                SwinLiteBlock(\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=shift_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    dropout=dropout,\n",
    "                    use_se=use_se\n",
    "                )\n",
    "            )\n",
    "\n",
    "        down = None\n",
    "        if downsample:\n",
    "            down = nn.ModuleDict({\n",
    "                \"ln\": nn.LayerNorm(dim),\n",
    "                \"conv\": nn.Conv2d(dim, dim * 2, kernel_size=2, stride=2)\n",
    "            })\n",
    "\n",
    "        return nn.ModuleDict({\n",
    "            \"blocks\": nn.ModuleList(blocks),\n",
    "            \"downsample\": down\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # x: (B, 3, H, W)\n",
    "        x = self.patch_embed(x)  # => (B, embed_dim, H//patch, W//patch)\n",
    "        B, C, H_, W_ = x.shape\n",
    "\n",
    "        # Flatten + transpose => (B, H'*W', embed_dim)\n",
    "        x = x.view(B, C, H_ * W_).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        curr_resolution = (H_, W_)\n",
    "        for stage in self.layers:\n",
    "            # Blocks\n",
    "            for blk in stage[\"blocks\"]:\n",
    "                x = blk(x)\n",
    "\n",
    "            # Downsample\n",
    "            if stage[\"downsample\"] is not None:\n",
    "                B_, N_, C_ = x.shape\n",
    "                h_, w_ = curr_resolution\n",
    "\n",
    "                # LN\n",
    "                x = stage[\"downsample\"][\"ln\"](x)\n",
    "\n",
    "                # Reshape to (B, C, h, w)\n",
    "                x = x.view(B_, h_, w_, C_).permute(0, 3, 1, 2)\n",
    "\n",
    "                # Conv2d => (B, 2*C, h//2, w//2)\n",
    "                x = stage[\"downsample\"][\"conv\"](x)\n",
    "\n",
    "                # Flatten => (B, h_*w_, 2*C)\n",
    "                h_, w_ = h_ // 2, w_ // 2\n",
    "                x = x.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "\n",
    "                curr_resolution = (h_, w_)\n",
    "\n",
    "        # Final norm + classification\n",
    "        x = self.norm(x)   # (B, N, final_dim)\n",
    "        x = x.mean(dim=1)  # => (B, final_dim)\n",
    "        x = self.head(x)   # => (B, num_classes)\n",
    "        return x\n",
    "\n",
    "#############################################################################\n",
    "# 10. Training / Validation / Test Routines\n",
    "#############################################################################\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    return test_acc\n",
    "\n",
    "#############################################################################\n",
    "# 11. Main\n",
    "#############################################################################\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    epochs = 100  # You can set more epochs if you have the resources (e.g., 200)\n",
    "    lr = 1e-3\n",
    "    weight_decay = 0.05\n",
    "\n",
    "    # Build a slightly larger SwinLite\n",
    "    model = SwinLite(\n",
    "        image_size=image_size,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        num_classes=200,\n",
    "        embed_dim=96,                # bigger embed\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],    # typical for Swin-T\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.0,\n",
    "        use_se=True\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Cosine Annealing LR Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"===== EPOCH {epoch+1} / {epochs} =====\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_swinlite_task_2.pth\")\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%  \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n\")\n",
    "\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "    # Evaluate on test set if available\n",
    "    if test_loader is not None:\n",
    "        model.load_state_dict(torch.load(\"best_swinlite.pth\"))\n",
    "        test_acc = test(model, test_loader, device)\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
