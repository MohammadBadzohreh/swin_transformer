{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pywt\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# If you have not installed lion_pytorch yet, install it via:\n",
    "# pip install lion-pytorch\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "\n",
    "# ------------------- Data Loading ------------------- \n",
    "image_size = 224\n",
    "\n",
    "tiny_transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tiny_transform_val = transforms.Compose([\n",
    "    transforms.Resize(72),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tiny_transform_test = transforms.Compose([\n",
    "    transforms.Resize(72),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = get_tinyimagenet_dataloaders(\n",
    "    data_dir='../datasets',\n",
    "    transform_train=tiny_transform_train,\n",
    "    transform_val=tiny_transform_val,\n",
    "    transform_test=tiny_transform_test,\n",
    "    batch_size=64,\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Two wavelet modules:\n",
    "#   1) LearnableDWTDownsample => stride=2, kernel_size=2 (used for stem & pools)\n",
    "#   2) LearnableDWTNoDownsample => stride=1, kernel_size=3, padding=1 (used in blocks)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "class LearnableDWTDownsample(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard wavelet downsampling with kernel=2, stride=2, groups=in_channels.\n",
    "    This halves spatial dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels * 4,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            groups=in_channels\n",
    "        )\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Use standard Haar wavelet for downsampling.\"\"\"\n",
    "        haar_low = torch.tensor([1.0, 1.0]) / np.sqrt(2)\n",
    "        haar_high = torch.tensor([-1.0, 1.0]) / np.sqrt(2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.zero_()\n",
    "            for c in range(self.in_channels):\n",
    "                for subband in range(4):\n",
    "                    oc = c * 4 + subband\n",
    "                    if subband == 0:      # LL\n",
    "                        w0, w1 = haar_low[0], haar_low[1]\n",
    "                    elif subband == 1:    # LH\n",
    "                        w0, w1 = haar_low[0], haar_high[1]\n",
    "                    elif subband == 2:    # HL\n",
    "                        w0, w1 = haar_high[0], haar_low[1]\n",
    "                    else:                 # HH\n",
    "                        w0, w1 = haar_high[0], haar_high[1]\n",
    "\n",
    "                    self.conv.weight[oc, 0, 0, 0] = w0\n",
    "                    self.conv.weight[oc, 0, 0, 1] = w1\n",
    "                    self.conv.weight[oc, 0, 1, 0] = w0\n",
    "                    self.conv.weight[oc, 0, 1, 1] = w1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        returns (B, C, H/2, W/2, 4)\n",
    "        \"\"\"\n",
    "        x = self.conv(x)  # => (B, 4*C, H/2, W/2)\n",
    "        return rearrange(x, 'b (c g) h w -> b c h w g', g=4)\n",
    "\n",
    "\n",
    "class LearnableDWTNoDownsample(nn.Module):\n",
    "    \"\"\"\n",
    "    Wavelet-like transform but WITHOUT reducing spatial resolution.\n",
    "    We use kernel=3, stride=1, padding=1 to preserve (H, W).\n",
    "    Groups = in_channels => each channel has a set of 4 wavelet filters.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels * 4,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=in_channels\n",
    "        )\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Place the 2x2 Haar kernel in the top-left corner, zero elsewhere.\"\"\"\n",
    "        haar_low = torch.tensor([1.0, 1.0]) / np.sqrt(2)\n",
    "        haar_high = torch.tensor([-1.0, 1.0]) / np.sqrt(2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.zero_()\n",
    "            # shape => (4*in_channels, 1, 3, 3)\n",
    "            for c in range(self.in_channels):\n",
    "                for subband in range(4):\n",
    "                    oc = c * 4 + subband\n",
    "                    if subband == 0:      # LL\n",
    "                        w0, w1 = haar_low[0], haar_low[1]\n",
    "                    elif subband == 1:    # LH\n",
    "                        w0, w1 = haar_low[0], haar_high[1]\n",
    "                    elif subband == 2:    # HL\n",
    "                        w0, w1 = haar_high[0], haar_low[1]\n",
    "                    else:                 # HH\n",
    "                        w0, w1 = haar_high[0], haar_high[1]\n",
    "\n",
    "                    # We store them in the top-left 2x2 of the 3x3 kernel\n",
    "                    self.conv.weight[oc, 0, 0, 0] = w0\n",
    "                    self.conv.weight[oc, 0, 0, 1] = w1\n",
    "                    self.conv.weight[oc, 0, 1, 0] = w0\n",
    "                    self.conv.weight[oc, 0, 1, 1] = w1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        returns (B, C, H, W, 4) same spatial size, but 4 wavelet subbands.\n",
    "        \"\"\"\n",
    "        x = self.conv(x)  # => (B, 4*C, H, W)\n",
    "        return rearrange(x, 'b (c g) h w -> b c h w g', g=4)\n",
    "\n",
    "\n",
    "# ------------------- WaveletAttention -------------------\n",
    "class WaveletAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.hf_gate = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, N, C)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        # => (3, B, num_heads, N, C//num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # High-frequency gating\n",
    "        hf_weight = torch.sigmoid(self.hf_gate)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x * hf_weight)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ------------------- Model Architecture -------------------\n",
    "class WOCBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A block that does a wavelet transform *without downsampling*,\n",
    "    merges LL and HF, then attention + FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WaveletAttention(dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Wavelet transform that does NOT reduce spatial size\n",
    "        self.dwt_no_down = LearnableDWTNoDownsample(in_channels=dim)\n",
    "        self.hf_gate = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, C).  Suppose N = H*W is square.\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        side = int(N**0.5)  # assume square\n",
    "\n",
    "        # Reshape tokens to (B, C, H, W)\n",
    "        x_img = rearrange(x, 'b (h w) c -> b c h w', h=side, w=side)\n",
    "\n",
    "        # Wavelet => shape (B, C, H, W, 4)\n",
    "        coeffs = self.dwt_no_down(x_img)\n",
    "        ll, lh, hl, hh = torch.chunk(coeffs, 4, dim=-1)  # each => (B, C, H, W, 1)\n",
    "\n",
    "        # Sum HF subbands => (B, C, H, W, 1)\n",
    "        hf_combined = lh + hl + hh\n",
    "\n",
    "        # Convert to tokens\n",
    "        ll_tokens = rearrange(ll, 'b c h w 1 -> b (h w) c')\n",
    "        hf_tokens = rearrange(hf_combined, 'b c h w 1 -> b (h w) c')\n",
    "        hf_tokens = hf_tokens * torch.sigmoid(self.hf_gate)\n",
    "\n",
    "        # Merge LL + HF => same number of tokens => (B, N, C)\n",
    "        x = ll_tokens + hf_tokens\n",
    "\n",
    "        # Attention + FFN\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class WOCSwin(nn.Module):\n",
    "    \"\"\"\n",
    "    A wavelet-based hierarchical model:\n",
    "      - Wavelet downsample \"stem\" => halved spatial resolution\n",
    "      - A few 'stages', each containing multiple WOCBlocks that *do not* reduce resolution\n",
    "      - A wavelet downsample \"pool\" between stages\n",
    "      - Finally, classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "        # 1) Stem => wavelet downsample from 3 channels to 3*4, halving spatial size\n",
    "        self.stem = LearnableDWTDownsample(in_channels=3)\n",
    "\n",
    "        # Then flatten => tokens & embed\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(12, 96),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 2) Stages: each stage has blocks that keep the same resolution\n",
    "        # and at the end we do wavelet \"pool\" to half resolution\n",
    "        self.stages = nn.ModuleList([\n",
    "            nn.Sequential(*[WOCBlock(dim=96,  num_heads=4) for _ in range(2)]),\n",
    "            nn.Sequential(*[WOCBlock(dim=192, num_heads=4) for _ in range(2)]),\n",
    "            nn.Sequential(*[WOCBlock(dim=384, num_heads=4) for _ in range(6)]),\n",
    "            nn.Sequential(*[WOCBlock(dim=768, num_heads=4) for _ in range(2)])\n",
    "        ])\n",
    "        \n",
    "        # 3) Pools => wavelet downsample between stages\n",
    "        self.pools = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                LearnableDWTDownsample(in_channels=96),\n",
    "                nn.Linear(96 * 4, 192)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                LearnableDWTDownsample(in_channels=192),\n",
    "                nn.Linear(192 * 4, 384)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                LearnableDWTDownsample(in_channels=384),\n",
    "                nn.Linear(384 * 4, 768)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # 4) Classifier head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(768, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W), e.g. H=W=64 for TinyImageNet\n",
    "        \"\"\"\n",
    "        # -- Stem -- \n",
    "        # out => (B, 3, H/2, W/2, 4), rearr => (B, (H/2)*(W/2), 12)\n",
    "        x = self.stem(x)  \n",
    "        x = rearrange(x, 'b c h w g -> b (h w) (c g)')  # => e.g. (B, 32*32=1024, 12)\n",
    "        x = self.embed(x)  # => (B, 1024, 96)\n",
    "\n",
    "        # -- Stages 0..2, each followed by wavelet pool --\n",
    "        for stage, pool_layer in zip(self.stages[:-1], self.pools):\n",
    "            x = stage(x)  # blocks keep resolution (no stride)\n",
    "            x = wavelet_pool(x, pool_layer)\n",
    "\n",
    "        # -- Final stage (no pool after) --\n",
    "        x = self.stages[-1](x)  # => dimension 768\n",
    "\n",
    "        # -- Head --\n",
    "        x = x.transpose(1, 2)  # => (B, 768, N)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def wavelet_pool(x, pool_module):\n",
    "    \"\"\"\n",
    "    pool_module = [LearnableDWTDownsample(in_channels=X), Linear(X*4, Y)]\n",
    "    Steps:\n",
    "      - reshape tokens => (B, X, H, W)\n",
    "      - wavelet downsample => (B, X, H/2, W/2, 4)\n",
    "      - flatten => (B, (H/2)*(W/2), X*4)\n",
    "      - linear => (B, (H/2)*(W/2), Y)\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    side = int(N**0.5)  # must be square => no mismatch\n",
    "    x_img = rearrange(x, 'b (h w) c -> b c h w', h=side, w=side)\n",
    "\n",
    "    dwt_layer, linear_layer = pool_module\n",
    "    y = dwt_layer(x_img)  # => (B, X, H/2, W/2, 4)\n",
    "    y = rearrange(y, 'b c h w g -> b (h w) (c g)')  # => (B, (H/2)*(W/2), X*4)\n",
    "    y = linear_layer(y)   # => (B, new_N, new_dim)\n",
    "    return y\n",
    "\n",
    "# ------------------- Training Loop -------------------\n",
    "class ProgressiveTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        # Lion optimizer\n",
    "        self.optimizer = Lion(self.model.parameters(), lr=3e-4, betas=(0.95, 0.98))\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=300)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def mix_features(self, x, epoch):\n",
    "        \"\"\"Progressive wavelet/noise mixing as data augmentation.\"\"\"\n",
    "        if epoch < 50:\n",
    "            ratio = 0.2 * (epoch / 50)\n",
    "        elif epoch < 150:\n",
    "            ratio = 0.2 + 0.8 * ((epoch - 50) / 100)\n",
    "        else:\n",
    "            ratio = 1.0 + 0.5 * ((epoch - 150) / 150)\n",
    "        \n",
    "        # Apply high-frequency noise\n",
    "        x = x + ratio * torch.randn_like(x) * 0.3\n",
    "        return x\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for images, labels in self.train_loader:\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Progressive mixing\n",
    "            images = self.mix_features(images, epoch)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        return total_loss / len(self.train_loader), correct / len(self.train_loader.dataset)\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.val_loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        return total_loss / len(self.val_loader), correct / len(self.val_loader.dataset)\n",
    "\n",
    "    def train(self, epochs=300):\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc*100:.2f}%\")\n",
    "            print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc*100:.2f}%\\n\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), \"woc_swin_best.pth\")\n",
    "\n",
    "\n",
    "# ------------------- Main Execution -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Detect device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = WOCSwin(num_classes=200)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ProgressiveTrainer(model, train_loader, val_loader, device)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(epochs=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
