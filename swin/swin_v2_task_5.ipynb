{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/100000] Loss: 5.338192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 296\u001b[0m\n\u001b[0;32m    294\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Optional: test a forward pass with a random input\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 250\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(model, device, train_loader, optimizer, epoch):\n\u001b[0;32m    249\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1280\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torchvision\\transforms\\functional.py:934\u001b[0m, in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madjust_hue\u001b[39m(img: Tensor, hue_factor: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    935\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Adjust hue of an image.\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \n\u001b[0;32m    937\u001b[0m \u001b[38;5;124;03m    The image hue is adjusted by converting the image to HSV and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Hue adjusted image.\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import torchvision.transforms as transforms\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "\n",
    "# Enhanced Data Augmentation\n",
    "image_size = 224\n",
    "tiny_transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((image_size + 20, image_size + 20)),\n",
    "    transforms.RandomCrop(image_size, padding=8, padding_mode='reflect'),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), value='random'),\n",
    "])\n",
    "\n",
    "tiny_transform_val = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_loader, val_loader, test_loader = get_tinyimagenet_dataloaders(\n",
    "    data_dir='../datasets',\n",
    "    transform_train=tiny_transform_train,\n",
    "    transform_val=tiny_transform_val,\n",
    "    transform_test=tiny_transform_val,\n",
    "    batch_size=64,\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model components\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Compact Patch Embedding with Depth-wise Convolution\"\"\"\n",
    "    def __init__(self, img_size=64, patch_size=4, in_chans=3, embed_dim=48):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.GroupNorm(4, embed_dim)\n",
    "        )\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    \"\"\"Depth-wise Conv Enhanced Window Attention with padding support\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, window_size=4, shift_size=0, groups=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.num_heads = num_heads\n",
    "        self.groups = groups\n",
    "\n",
    "        # Depth-wise local feature enhancement\n",
    "        self.dw_conv = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.norm = nn.GroupNorm(groups, dim)\n",
    "        \n",
    "        # Sparse attention parameters\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, L, C = x.shape\n",
    "        # Reshape to spatial map: [B, H, W, C]\n",
    "        x = x.view(B, H, W, C)\n",
    "        \n",
    "        # Pad H and W if they are not divisible by window_size\n",
    "        pad_h = (self.window_size - H % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - W % self.window_size) % self.window_size\n",
    "        if pad_h or pad_w:\n",
    "            # For F.pad we need (pad_left, pad_right, pad_top, pad_bottom)\n",
    "            # Here we pad width (last dim) and height (second-to-last dim).\n",
    "            x = x.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
    "            x = x.permute(0, 2, 3, 1)  # [B, H_pad, W_pad, C]\n",
    "            H_pad, W_pad = H + pad_h, W + pad_w\n",
    "        else:\n",
    "            H_pad, W_pad = H, W\n",
    "\n",
    "        # Depth-wise convolution enhancement\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, C, H_pad, W_pad]\n",
    "        x = self.dw_conv(x) + x\n",
    "        # Apply normalization: first permute to [B, H_pad, W_pad, C]\n",
    "        x = self.norm(x).permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        # Optional shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        \n",
    "        # Window partitioning\n",
    "        x = x.view(B,\n",
    "                   H_pad // self.window_size, self.window_size,\n",
    "                   W_pad // self.window_size, self.window_size,\n",
    "                   C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        # Grouped attention computation\n",
    "        qkv = self.qkv(x).view(-1, self.window_size * self.window_size, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(-1, self.window_size * self.window_size, C)\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Window reconstruction\n",
    "        x = x.view(-1,\n",
    "                   H_pad // self.window_size, W_pad // self.window_size,\n",
    "                   self.window_size, self.window_size, C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H_pad, W_pad, C)\n",
    "        \n",
    "        # Remove any padding if added\n",
    "        if pad_h or pad_w:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "        \n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        \n",
    "        return x.view(B, L, C)\n",
    "class GroupMLP(nn.Module):\n",
    "    \"\"\"Parameter-efficient Grouped MLP\"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, groups=2):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features * 2\n",
    "        self.groups = groups\n",
    "        self.fc1 = nn.Conv1d(in_features, hidden_features, 1, groups=groups)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Conv1d(hidden_features, in_features, 1, groups=groups)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "class SlimSwinBlock(nn.Module):\n",
    "    \"\"\"Compact Transformer Block\"\"\"\n",
    "    def __init__(self, dim, num_heads, window_size=4, shift_size=0, groups=2):\n",
    "        super().__init__()\n",
    "        self.attn_norm = nn.GroupNorm(groups, dim)\n",
    "        self.attn = HybridAttention(dim, num_heads, window_size, shift_size, groups)\n",
    "        self.mlp_norm = nn.GroupNorm(groups, dim)\n",
    "        self.mlp = GroupMLP(dim, groups=groups)\n",
    "        \n",
    "    def forward(self, x, H, W):\n",
    "        # Apply norm on [B, L, C]: permute to [B, C, L], normalize, then permute back.\n",
    "        x = x + self.attn(self.attn_norm(x.permute(0, 2, 1)).permute(0, 2, 1), H, W)\n",
    "        x = x + self.mlp(self.mlp_norm(x.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "        return x\n",
    "\n",
    "class SlimSwin(nn.Module):\n",
    "    \"\"\"Slim Swin Transformer Architecture\"\"\"\n",
    "    def __init__(self, img_size=64, patch_size=4, in_chans=3, num_classes=200,\n",
    "                 depths=[2, 2, 2], dims=[48, 96, 192], num_heads=[4, 8, 16], \n",
    "                 window_size=4, groups=2):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        current_dim = dims[0]\n",
    "        \n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, current_dim)\n",
    "        self.pos_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        # Build transformer and merging stages\n",
    "        for i, depth in enumerate(depths):\n",
    "            # Transformer blocks stage\n",
    "            stage = nn.Sequential(\n",
    "                *[SlimSwinBlock(\n",
    "                    dim=dims[i],\n",
    "                    num_heads=num_heads[i],\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    groups=groups\n",
    "                ) for _ in range(depth)]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            \n",
    "            if i != len(depths) - 1:\n",
    "                # Efficient patch merging stage\n",
    "                merge = nn.Sequential(\n",
    "                    nn.GroupNorm(groups, dims[i]),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], 3, stride=2, padding=1, groups=groups)\n",
    "                )\n",
    "                self.stages.append(merge)\n",
    "                \n",
    "        self.norm = nn.GroupNorm(groups, dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # [B, L, C]\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L ** 0.5)  # e.g., 28x28 for input images of size 224\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for stage in self.stages:\n",
    "            # Determine if stage is a transformer block or a merging stage.\n",
    "            if hasattr(stage[0], '__class__') and stage[0].__class__.__name__ == \"SlimSwinBlock\":\n",
    "                # For transformer blocks, iterate over each block with H and W.\n",
    "                for block in stage:\n",
    "                    x = block(x, H, W)\n",
    "            else:\n",
    "                # Patch merging stage: reshape, process, then flatten back.\n",
    "                x = x.view(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "                x = stage(x)\n",
    "                B, C, H, W = x.shape\n",
    "                x = x.view(B, C, -1).permute(0, 2, 1)  # [B, L, C]\n",
    "        x = self.norm(x.mean(dim=1))\n",
    "        return self.head(x)\n",
    "\n",
    "# Training and evaluation routines\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Avg loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "          f\"({100. * correct / len(test_loader.dataset):.0f}%)\\n\")\n",
    "\n",
    "# Main script\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use CUDA if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = SlimSwin(\n",
    "        img_size=64,  # Note: this parameter is used for grid computation in patch embedding.\n",
    "        patch_size=4,\n",
    "        num_classes=200,\n",
    "        depths=[2, 2, 2],\n",
    "        dims=[48, 96, 192],\n",
    "        num_heads=[4, 8, 16],\n",
    "        groups=4\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)\n",
    "    \n",
    "    num_epochs = 200\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "    \n",
    "    # Optional: test a forward pass with a random input\n",
    "    x = torch.randn(2, 3, 64, 64).to(device)\n",
    "    output = model(x)\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "    print(f\"Output shape: {output.shape}\")  # Expected: (2, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
